import sys
from langchain_core.tools import tool
from typing import Optional, Dict, List, Any, Union
import mongo.constants
import os
import json
import re
from glob import glob
from datetime import datetime
from orchestrator import Orchestrator, StepSpec, as_async
from qdrant.initializer import RAGTool
# Qdrant and RAG dependencies
# try:
#     from qdrant_client import QdrantClient
#     from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
#     from sentence_transformers import SentenceTransformer
#     import numpy as np
# except ImportError:
#     QdrantClient = None
#     SentenceTransformer = None
#     np = None

mongodb_tools = mongo.constants.mongodb_tools
DATABASE_NAME = mongo.constants.DATABASE_NAME
try:
    from planner import plan_and_execute_query
except ImportError:
    plan_and_execute_query = None


# ------------------ RAG Retrieval Defaults ------------------
# Per content_type default limits for retrieval. These are applied when the caller
# does not explicitly provide a limit (i.e., limit is None).
# Rationale: pages/work_items generally require broader recall; projects/cycles/modules
# tend to be fewer and more concise.
CONTENT_TYPE_DEFAULT_LIMITS: Dict[str, int] = {
    "page": 12,
    "work_item": 12,
    "project": 6,
    "cycle": 6,
    "module": 6,
    "epic": 6,
}

# Fallback when content_type is unknown or not provided
DEFAULT_RAG_LIMIT: int = 10

# Optional: per content_type chunk-level tuning for chunk-aware retrieval
# - chunks_per_doc controls how many high-scoring chunks are kept per reconstructed doc
# - include_adjacent controls whether to pull neighboring chunks for context
# - min_score sets a score threshold for initial vector hits
CONTENT_TYPE_CHUNKS_PER_DOC: Dict[str, int] = {
    "page": 3,          # Reduced from 4 to minimize context window usage
    "work_item": 4,     # Reduced from 3 to minimize context window usage
    "project": 2,
    "cycle": 2,
    "module": 2,
    "epic": 2,
}

CONTENT_TYPE_INCLUDE_ADJACENT: Dict[str, bool] = {
    "page": True,      # Disabled to reduce context window usage (was True)
    "work_item": True,  # Keep adjacent for work items for better context
    "project": False,
    "cycle": False,
    "module": False,
    "epic": False,
}

CONTENT_TYPE_MIN_SCORE: Dict[str, float] = {
    "page": 0.5,
    "work_item": 0.5,
    "project": 0.55,
    "cycle": 0.55,
    "module": 0.55,
    "epic": 0.55,
}


def normalize_mongodb_types(obj: Any) -> Any:
    """Convert MongoDB extended JSON types to regular Python types."""
    if obj is None:
        return None

    if isinstance(obj, dict):
        # Handle MongoDB-specific types
        if '$binary' in obj:
            # Convert binary to string representation (we'll filter it out anyway)
            return f"<binary:{obj['$binary']['base64'][:8]}...>"
        elif '$date' in obj:
            # Convert MongoDB date to string representation
            return obj['$date']
        elif '$oid' in obj:
            # Convert ObjectId to string
            return obj['$oid']
        else:
            # Recursively process nested objects
            return {key: normalize_mongodb_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        # Process lists recursively
        return [normalize_mongodb_types(item) for item in obj]
    else:
        # Return primitive types as-is
        return obj


def filter_meaningful_content(data: Any) -> Any:
    """Filter MongoDB documents to keep only meaningful content fields.

    Removes unnecessary fields like _id, timestamps, and other metadata
    while preserving actual content like text, names, descriptions, etc.

    Args:
        data: Raw MongoDB document(s) - can be dict, list, or other types

    Returns:
        Filtered data with only meaningful content fields
    """
    # First, normalize MongoDB extended JSON to regular Python types
    normalized_data = normalize_mongodb_types(data)

    # Handle edge cases
    if normalized_data is None:
        return None

    # Define fields that contain meaningful content (not metadata)
    CONTENT_FIELDS = {
        # Text content
        'title', 'description', 'name', 'content', 'email', 'role',
        'priority', 'status', 'state', 'displayBugNo', 'projectDisplayId',
        # Business logic fields
        'label', 'type', 'access', 'visibility', 'icon', 'imageUrl',
        'business', 'staff', 'createdBy', 'assignee', 'project', 'cycle', 'module',
        'members', 'pages', 'projectStates', 'subStates', 'linkedCycle', 'linkedModule',
        # Date fields (but not timestamps)
        'startDate', 'endDate', 'joiningDate', 'createdAt', 'updatedAt',
        # Estimate and work tracking
        'estimate', 'estimateSystem', 'workLogs',
        # Count/aggregation results
        'total', 'count', 'group', 'items'
    }

    # Fields to always exclude (metadata)
    EXCLUDE_FIELDS = {
        '_id', 'createdTimeStamp', 'updatedTimeStamp',
        '_priorityRank',  # Helper field added by pipeline
        '_class',  # Drop Java class metadata
    }

    def is_meaningful_field(key: str, value: Any) -> bool:
        """Check if a field contains meaningful content."""
        # Always exclude metadata fields
        if key in EXCLUDE_FIELDS:
            return False

        # Keep content fields
        if key in CONTENT_FIELDS:
            return True

        # For unknown fields, check if they have meaningful values
        if isinstance(value, str) and value.strip():
            # Non-empty strings are meaningful
            return True
        elif isinstance(value, (int, float)) and not key.endswith(('Id', '_id')):
            # Numbers that aren't IDs are meaningful
            return True
        elif isinstance(value, bool):
            # Boolean values are meaningful
            return True
        elif isinstance(value, dict):
            # Recursively check nested objects
            return any(is_meaningful_field(k, v) for k, v in value.items())
        elif isinstance(value, list) and value:
            # Check if list contains meaningful content, including dict items
            for item in value:
                if isinstance(item, (str, int, float, bool)):
                    if not isinstance(item, str) or item.strip():
                        return True
                elif isinstance(item, dict):
                    if any(is_meaningful_field(k, v) for k, v in item.items()):
                        return True
            return False

        return False

    def clean_document(doc: Any) -> Any:
        """Clean a single document or value."""
        if isinstance(doc, dict):
            # Filter dictionary
            cleaned = {}
            for key, value in doc.items():
                if is_meaningful_field(key, value):
                    if isinstance(value, (dict, list)):
                        cleaned_value = clean_document(value)
                        if cleaned_value:  # Only add if there's meaningful content
                            cleaned[key] = cleaned_value
                    else:
                        cleaned[key] = value
            return cleaned if cleaned else {}
        elif isinstance(doc, list):
            # Filter list of documents
            cleaned = []
            for item in doc:
                cleaned_item = clean_document(item)
                if cleaned_item:  # Only add if there's meaningful content
                    cleaned.append(cleaned_item)
            return cleaned
        else:
            # Return primitive values as-is
            return doc

    return clean_document(normalized_data)


def _is_hex_object_id(value: str) -> bool:
    try:
        return isinstance(value, str) and len(value) == 24 and all(c in '0123456789abcdefABCDEF' for c in value)
    except Exception:
        return False


def _is_binary_placeholder(value: Any) -> bool:
    return isinstance(value, str) and value.startswith("<binary:")


def _is_uuid_string(value: Any) -> bool:
    if not isinstance(value, str):
        return False
    # Simple UUID v4-like pattern check
    if len(value) != 36:
        return False
    parts = value.split("-")
    if len(parts) != 5:
        return False
    expected_lengths = [8, 4, 4, 4, 12]
    for part, L in zip(parts, expected_lengths):
        if len(part) != L:
            return False
        if not all(c in '0123456789abcdefABCDEF' for c in part):
            return False
    return True


def _is_id_like_key(key: str) -> bool:
    # Allowlist display ids
    ALLOWLIST = {"projectDisplayId"}
    if key in ALLOWLIST:
        return False
    lowered = key.lower()
    return (
        key == "_id"
        or lowered == "id"
        or lowered.endswith("id")  # memberId, projectId, defaultAsigneeId, etc.
        or lowered.endswith("_id")
        or lowered.endswith("uuid")
    )


def _strip_ids(value: Any) -> Any:
    """Recursively remove id/uuid-like fields and raw id values from documents."""
    if isinstance(value, dict):
        cleaned: Dict[str, Any] = {}
        for k, v in value.items():
            if _is_id_like_key(k):
                # drop id-like keys entirely
                continue
            # Recurse first
            v2 = _strip_ids(v)
            # Drop values that are just IDs or binary placeholders
            if isinstance(v2, str) and (_is_hex_object_id(v2) or _is_uuid_string(v2) or _is_binary_placeholder(v2)):
                continue
            if v2 is None:
                continue
            # Drop empty containers
            if isinstance(v2, (dict, list)) and not v2:
                continue
            cleaned[k] = v2
        return cleaned
    if isinstance(value, list):
        items = [_strip_ids(x) for x in value]
        items = [x for x in items if x not in (None, {}) and not (isinstance(x, str) and (_is_hex_object_id(x) or _is_uuid_string(x) or _is_binary_placeholder(x)))]
        return items
    return value


def _ensure_list_of_names(obj: Any) -> List[str]:
    names: List[str] = []
    if isinstance(obj, list):
        for item in obj:
            if isinstance(item, dict):
                name = item.get("name") or item.get("title")
                if isinstance(name, str) and name.strip():
                    names.append(name)
            elif isinstance(item, str) and item.strip() and not _is_hex_object_id(item) and not _is_binary_placeholder(item):
                names.append(item)
    elif isinstance(obj, dict):
        name = obj.get("name") or obj.get("title")
        if isinstance(name, str) and name.strip():
            names.append(name)
    elif isinstance(obj, str) and obj.strip():
        names.append(obj)
    return names


def _transform_by_collection(doc: Dict[str, Any], collection: Optional[str]) -> Dict[str, Any]:
    if not isinstance(doc, dict):
        return doc  # type: ignore[return-value]

    collection = (collection or "").strip()
    out: Dict[str, Any] = {}

    def copy_if_present(key: str, alias: Optional[str] = None):
        val = doc.get(key)
        if val is not None:
            out[alias or key] = val

    # Common flatteners
    def set_name(source_key: str, target_key: str):
        val = doc.get(source_key)
        if isinstance(val, dict):
            name = val.get("name") or val.get("title")
            if name:
                out[target_key] = name

    def set_names_list(source_key: str, target_key: str):
        val = doc.get(source_key)
        names = _ensure_list_of_names(val)
        if names:
            out[target_key] = names

    # Always useful common keys
    for k in ["title", "name", "description", "status", "priority", "label",
              "visibility", "access", "imageUrl", "icon",
              "favourite", "isFavourite", "isActive", "isArchived",
              "content", "displayBugNo", "projectDisplayId",
              "startDate", "endDate", "createdAt", "updatedAt",
              "estimate", "estimateSystem", "workLogs"]:
        if k in doc:
            out[k] = doc[k]

    # Per collection enrichments
    if collection == "workItem":
        set_name("project", "projectName")
        set_name("state", "stateName")
        set_name("stateMaster", "stateMasterName")
        set_name("cycle", "cycleName")
        # modules in schema is a single subdoc despite plural key
        set_name("modules", "moduleName")
        set_name("business", "businessName")
        set_name("createdBy", "createdByName")
        set_names_list("assignee", "assignees")
        set_names_list("updatedBy", "updatedByNames")

    elif collection == "project":
        set_name("business", "businessName")
        set_name("lead", "leadName")
        set_name("defaultAsignee", "defaultAssigneeName")
        set_name("createdBy", "createdByName")
        copy_if_present("leadMail")

    elif collection == "cycle":
        # Project may only contain id; we skip if name isn't present
        set_name("project", "projectName")
        set_name("business", "businessName")

    elif collection == "module":
        set_name("project", "projectName")
        set_name("lead", "leadName")
        set_name("business", "businessName")
        set_names_list("assignee", "assignees")

    elif collection == "members":
        set_name("project", "projectName")
        set_name("staff", "staffName")

    elif collection == "page":
        set_name("project", "projectName")
        set_name("createdBy", "createdByName")
        set_name("business", "businessName")
        # Linked arrays could contain ids only; surface counts
        if isinstance(doc.get("linkedCycle"), list):
            out["linkedCycleCount"] = len(doc["linkedCycle"])  # type: ignore[index]
        if isinstance(doc.get("linkedModule"), list):
            out["linkedModuleCount"] = len(doc["linkedModule"])  # type: ignore[index]
        # Also surface names if available
        set_names_list("linkedCycle", "linkedCycleNames")
        set_names_list("linkedModule", "linkedModuleNames")

    elif collection == "projectState":
        # Keep core fields and slim subStates
        substates = doc.get("subStates")
        if isinstance(substates, list):
            slim: List[Dict[str, Any]] = []
            for s in substates:
                if isinstance(s, dict):
                    entry: Dict[str, Any] = {}
                    if isinstance(s.get("name"), str):
                        entry["name"] = s["name"]
                    if isinstance(s.get("order"), (int, float)):
                        entry["order"] = s["order"]
                    if entry:
                        slim.append(entry)
            if slim:
                out["subStates"] = slim

    elif collection == "timeline":
        # Surface friendly names and key attributes
        set_name("project", "projectName")
        # user is actor
        set_name("user", "actorName")
        # include work item title if present
        if isinstance(doc.get("workItemTitle"), str):
            out["workItemTitle"] = doc["workItemTitle"]
        # event type and field changed are useful summarizers
        if isinstance(doc.get("type"), str):
            out["timelineType"] = doc["type"]
        if isinstance(doc.get("fieldChanged"), str):
            out["fieldChanged"] = doc["fieldChanged"]

    # Drop empty/None values and metadata keys
    out = {k: v for k, v in out.items() if v not in (None, "", [], {}) and k != "_class"}
    return out


def filter_and_transform_content(data: Any, primary_entity: Optional[str] = None) -> Any:
    """Preserve meaningful fields, strip IDs/UUIDs, and flatten references per collection.

    Steps:
    1) Use existing filter to keep meaningful content fields.
    2) Strip any remaining id/uuid-like keys/values.
    3) Apply per-collection flatteners to surface human-friendly names.
    """
    base = filter_meaningful_content(data)
    stripped = _strip_ids(base)

    def enrich(obj: Any) -> Any:
        if isinstance(obj, dict):
            # Merge base with collection-specific projection
            extra = _transform_by_collection(obj, primary_entity)
            # Overlay extra on top of obj (extra wins)
            merged = {**obj, **extra}
            return {k: v for k, v in merged.items() if v not in (None, "", [], {})}
        return obj

    if isinstance(stripped, list):
        return [enrich(x) for x in stripped]
    if isinstance(stripped, dict):
        return enrich(stripped)
    return stripped


@tool
async def mongo_query(query: str, show_all: bool = False) -> str:
    """Plan-first Mongo query executor for structured, factual questions.

    Use this ONLY when the user asks for authoritative data that must come from
    MongoDB (counts, lists, filters, group-by, breakdowns, state/assignee/project details)
    across collections: `project`, `workItem`, `cycle`, `module`, `epic`, `members`,
    `page`, `projectState`, `timeline`.

    Do NOT use this for:
    - Free-form content questions (use `rag_search`).
    - Pure summarization or opinion without data retrieval.
    - When you already have the exact answer in prior tool results.

    Behavior:
    - Follows a planner to generate a safe aggregation pipeline; avoids
      hallucinated fields.
    - Automatically determines when complex joins are beneficial based on query requirements.
    - Intelligently adds strategic relationships only when they improve query performance:
        - Multi-hop queries: "work items by business" (workItemâ†’projectâ†’business)
        - Cross-collection analysis: "members working on projects by business"
        - Complex grouping that spans multiple collections
    - Only adds joins that provide clear benefits for the specific query, avoiding unnecessary complexity.

    Args:
        query: Natural language, structured data request about PM entities.
        show_all: If True, output full details instead of a summary. Use sparingly.

    Returns: A compact result suitable for direct user display.
    """
    if not plan_and_execute_query:
        return "âŒ Intelligent query planner not available. Please ensure query_planner.py is properly configured."

    try:
        result = await plan_and_execute_query(query)

        if result["success"]:
            response = f"ðŸŽ¯ INTELLIGENT QUERY RESULT:\n"
            response += f"Query: '{query}'\n\n"

            # Show parsed intent
            intent = result["intent"]
            response += f"ðŸ“‹ UNDERSTOOD INTENT:\n"
            if result.get("planner"):
                response += f"â€¢ Planner: {result['planner']}\n"
            response += f"â€¢ Primary Entity: {intent['primary_entity']}\n"
            if intent['target_entities']:
                response += f"â€¢ Related Entities: {', '.join(intent['target_entities'])}\n"
            if intent['filters']:
                response += f"â€¢ Filters: {intent['filters']}\n"
            if intent['aggregations']:
                response += f"â€¢ Aggregations: {', '.join(intent['aggregations'])}\n"
            response += "\n"

            # Show the generated pipeline (first few stages)
            pipeline = result.get("pipeline")
            pipeline_js = result.get("pipeline_js")
            if pipeline_js:
                response += f"ðŸ”§ GENERATED PIPELINE:\n"
                response += pipeline_js
                response += "\n"
            elif pipeline:
                response += f"ðŸ”§ GENERATED PIPELINE:\n"
                # Import the formatting function from planner
                try:
                    from planner import _format_pipeline_for_display
                    formatted_pipeline = _format_pipeline_for_display(pipeline)
                    response += formatted_pipeline
                except ImportError:
                    # Fallback to JSON format if import fails
                    for i, stage in enumerate(pipeline):
                        stage_name = list(stage.keys())[0]
                        # Format the stage content nicely
                        stage_content = json.dumps(stage[stage_name], indent=2)
                        # Truncate very long content for readability but show complete structure
                        if len(stage_content) > 200:
                            stage_content = stage_content + "..."
                        response += f"â€¢ {stage_name}: {stage_content}\n"
                response += "\n"

            # Show results (compact preview)
            rows = result.get("result")
            try:
                # Attempt to parse stringified JSON results
                if isinstance(rows, str):
                    parsed = json.loads(rows)
                else:
                    parsed = rows
            except Exception:
                parsed = rows

            # Handle the specific MongoDB response format
            if isinstance(parsed, list) and len(parsed) > 0:
                # Check if first element is a string (like "Found X documents...")
                if isinstance(parsed[0], str) and parsed[0].startswith("Found"):
                    # This is the MongoDB response format: [message, doc1_json, doc2_json, ...]
                    # Parse the JSON strings and filter them
                    documents = []
                    for item in parsed[1:]:  # Skip the first message
                        if isinstance(item, str):
                            try:
                                doc = json.loads(item)
                                filtered_doc = filter_meaningful_content(doc)
                                if filtered_doc:  # Only add if there's meaningful content
                                    documents.append(filtered_doc)
                            except Exception:
                                # Skip invalid JSON
                                continue
                        else:
                            # Already parsed, filter directly
                            filtered_doc = filter_meaningful_content(item)
                            if filtered_doc:
                                documents.append(filtered_doc)

                    filtered = documents
                else:
                    # Regular list, filter as before
                    filtered = parsed
            else:
                # Not a list, filter as before
                filtered = parsed


            def format_llm_friendly(data, max_items=50, primary_entity: Optional[str] = None):
                """Format data in a more LLM-friendly way to avoid hallucinations."""
                def get_nested(d: Dict[str, Any], key: str) -> Any:
                    if key in d:
                        return d[key]
                    if "." in key:
                        cur: Any = d
                        for part in key.split("."):
                            if isinstance(cur, dict) and part in cur:
                                cur = cur[part]
                            else:
                                return None
                        return cur
                    return None

                def ensure_list_str(val: Any) -> List[str]:
                    if isinstance(val, list):
                        res: List[str] = []
                        for x in val:
                            if isinstance(x, str) and x.strip():
                                res.append(x)
                            elif isinstance(x, dict):
                                n = x.get("name") or x.get("title")
                                if isinstance(n, str) and n.strip():
                                    res.append(n)
                        return res
                    if isinstance(val, dict):
                        n = val.get("name") or val.get("title")
                        return [n] if isinstance(n, str) and n.strip() else []
                    if isinstance(val, str) and val.strip():
                        return [val]
                    return []

                def truncate_str(s: Any, limit: int = 120) -> str:
                    if not isinstance(s, str):
                        return str(s)
                    return s if len(s) <= limit else s[:limit] + "..."

                def render_line(entity: Dict[str, Any]) -> str:
                    e = (primary_entity or "").lower()
                    if e == "workitem":
                        bug = entity.get("displayBugNo") or entity.get("title") or "Item"
                        title = entity.get("title") or entity.get("name") or ""
                        state = entity.get("stateName") or get_nested(entity, "state.name")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        assignees = ensure_list_str(entity.get("assignees") or entity.get("assignee"))
                        priority = entity.get("priority")
                        label = entity.get_nested(entity,"label.name")
                        # Build base line
                        base = f"â€¢ {bug}: {truncate_str(title, 80)} â€” state={state or 'N/A'}, priority={priority or 'N/A'}, assignee={(assignees[0] if assignees else 'N/A')}, project={project or 'N/A'}, label={label or 'N/A'}"
                        
                        # Add estimate if present
                        estimate = entity.get("estimate")
                        if estimate and isinstance(estimate, dict):
                            hr = estimate.get("hr", "0")
                            min_val = estimate.get("min", "0")
                            base += f", estimate={hr}h {min_val}m"
                        elif estimate:
                            base += f", estimate={estimate}"
                        
                        # Add work logs if present
                        work_logs = entity.get("workLogs")
                        if work_logs and isinstance(work_logs, list) and len(work_logs) > 0:
                            total_hours = sum(log.get("hours", 0) for log in work_logs if isinstance(log, dict))
                            total_mins = sum(log.get("minutes", 0) for log in work_logs if isinstance(log, dict))
                            total_hours += total_mins // 60
                            total_mins = total_mins % 60
                            base += f", logged={total_hours}h {total_mins}m ({len(work_logs)} logs)"
                            
                            descriptions = [
                                log.get("description", "").strip()
                                for log in work_logs
                                if isinstance(log, dict) and log.get("description")
                            ]
                            descriptions_text = "; ".join(descriptions) if descriptions else "No descriptions"

                            base += f", descriptions=[{descriptions_text}]"
                        return base
                    if e == "project":
                        pid = entity.get("projectDisplayId")
                        name = entity.get("name") or entity.get("title")
                        status_v = entity.get("status")
                        lead = entity.get("leadName") or get_nested(entity, "lead.name")
                        business = entity.get("businessName") or get_nested(entity, "business.name")
                        return f"â€¢ {pid or name}: {name or ''} â€” status={status_v or 'N/A'}, lead={lead or 'N/A'}, business={business or 'N/A'}"
                    if e == "cycle":
                        title = entity.get("title") or entity.get("name")
                        status_v = entity.get("status")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        sd = entity.get("startDate")
                        ed = entity.get("endDate")
                        dates = f"{sd} â†’ {ed}" if sd or ed else "N/A"
                        return f"â€¢ {truncate_str(title or 'Cycle', 80)} â€” status={status_v or 'N/A'}, project={project or 'N/A'}, dates={dates}"
                    if e == "module":
                        title = entity.get("title") or entity.get("name")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        assignees = ensure_list_str(entity.get("assignees") or entity.get("assignee"))
                        business = entity.get("businessName") or get_nested(entity, "business.name")
                        return f"â€¢ {truncate_str(title or 'Module', 80)} â€” project={project or 'N/A'}, assignees={(len(assignees) if assignees else 0)}, business={business or 'N/A'}"
                    if e == "members":
                        name = entity.get("name")
                        email = entity.get("email")
                        role = entity.get("role")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        type_v = entity.get("type")
                        return f"â€¢ {name or 'Member'} â€” role={role or 'N/A'}, email={email or 'N/A'}, type={type_v or 'N/A'}, project={project or 'N/A'}"
                    if e == "page":
                        title = entity.get("title") or entity.get("name")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        visibility = entity.get("visibility")
                        fav = entity.get("isFavourite")
                        return f"â€¢ {truncate_str(title or 'Page', 80)} â€” visibility={visibility or 'N/A'}, favourite={fav if fav is not None else 'N/A'}, project={project or 'N/A'}"
                    if e == "projectstate":
                        name = entity.get("name")
                        icon = entity.get("icon")
                        subs = entity.get("subStates")
                        sub_count = len(subs) if isinstance(subs, list) else 0
                        return f"â€¢ {name or 'State'} â€” icon={icon or 'N/A'}, substates={sub_count}"
                    
                    if e == "epic":
                        title = entity.get("title") or entity.get("name")
                        description = entity.get("description")
                        state = entity.get("stateName") or get_nested(entity, "state.name")
                        priority = entity.get("priority")
                        assignee = entity.get("assigneeName") or get_nested(entity, "assignee.name")
                        project = entity.get("projectName") or get_nested(entity, "project.name")
                        bug_number = entity.get("bugNo")
                        label_name = get_nested(entity, "label.name")
                        return (
                            f"â€¢ {truncate_str(title or 'Epic', 80)} â€” "
                            f"state={state or 'N/A'}, priority={priority or 'N/A'}, "
                            f"project={project or 'N/A'}, assignee={assignee or 'N/A'}, "
                            f"bugNo={bug_number or 'N/A'}, label={label_name or 'N/A'}"
                        )
                    # Default fallback
                    title = entity.get("title") or entity.get("name") or "Item"
                    return f"â€¢ {truncate_str(title, 80)}"
                if isinstance(data, list):
                    # Handle count-only results
                    if len(data) == 1 and isinstance(data[0], dict) and "total" in data[0]:
                        return f"ðŸ“Š RESULTS:\nTotal: {data[0]['total']}"

                    # Handle grouped/aggregated results
                    if len(data) > 0 and isinstance(data[0], dict) and ("count" in data[0] or "totalMinutes" in data[0]):
                        response = "ðŸ“Š RESULTS SUMMARY:\n"
                        # Prefer minutes total when available, else use count
                        has_minutes = any('totalMinutes' in item for item in data)
                        total_items = sum(item.get('count', 0) for item in data)
                        total_minutes = sum(item.get('totalMinutes', 0) for item in data) if has_minutes else None

                        # Determine what type of grouping this is
                        first_item = data[0]
                        group_keys = [k for k in first_item.keys() if k not in ['count', 'items', 'totalMinutes']]

                        if group_keys:
                            if has_minutes and total_minutes is not None:
                                response += f"Found {len(data)} groups grouped by {', '.join(group_keys)} (total {int(total_minutes)} min):\n\n"
                            else:
                                response += f"Found {total_items} items grouped by {', '.join(group_keys)}:\n\n"

                            # Sort by count (highest first) and show more groups
                            if has_minutes:
                                sorted_data = sorted(data, key=lambda x: x.get('totalMinutes', 0), reverse=True)
                            else:
                                sorted_data = sorted(data, key=lambda x: x.get('count', 0), reverse=True)

                            # Show all groups if max_items is None, otherwise limit
                            display_limit = len(sorted_data) if max_items is None else 25
                            for item in sorted_data[:display_limit]:
                                group_values = [f"{k}: {item[k]}" for k in group_keys if k in item]
                                group_label = ', '.join(group_values)
                                if has_minutes:
                                    mins = int(item.get('totalMinutes', 0) or 0)
                                    response += f"â€¢ {group_label}: {mins} min\n"
                                else:
                                    count = item.get('count', 0)
                                    response += f"â€¢ {group_label}: {count} items\n"

                            if max_items is not None and len(data) > 25:
                                if has_minutes:
                                    remaining = sum(int(item.get('totalMinutes', 0) or 0) for item in sorted_data[25:])
                                    response += f"â€¢ ... and {len(data) - 25} other categories: {remaining} min\n"
                                else:
                                    remaining = sum(item.get('count', 0) for item in sorted_data[25:])
                                    response += f"â€¢ ... and {len(data) - 25} other categories: {remaining} items\n"
                            elif max_items is None and len(data) > display_limit:
                                if has_minutes:
                                    remaining = sum(int(item.get('totalMinutes', 0) or 0) for item in sorted_data[display_limit:])
                                    response += f"â€¢ ... and {len(data) - display_limit} other categories: {remaining} min\n"
                                else:
                                    remaining = sum(item.get('count', 0) for item in sorted_data[display_limit:])
                                    response += f"â€¢ ... and {len(data) - display_limit} other categories: {remaining} items\n"
                        else:
                            if has_minutes and total_minutes is not None:
                                response += f"Found total {int(total_minutes)} min\n"
                            else:
                                response += f"Found {total_items} items\n"
                        print(response)
                        return response

                    # Handle list of documents - show summary instead of raw JSON
                    if max_items is not None and len(data) > max_items:
                        response = f"ðŸ“Š RESULTS SUMMARY:\n"
                        response += f"Found {len(data)} items. Showing key details for last {max_items}:\n\n"
                        # Show sample items in a collection-aware way
                        for i, item in enumerate(data[-max_items:], len(data) - max_items + 1):
                            if isinstance(item, dict):
                                response += render_line(item) + "\n"
                        if len(data) > max_items:
                            response += f"â€¢ ... and {len(data) - max_items} items were omitted above\n"
                        return response
                    else:
                        # Show all items or small list - show in formatted way
                        response = "ðŸ“Š RESULTS:\n"
                        for item in data:
                            if isinstance(item, dict):
                                response += render_line(item) + "\n"
                        return response

                # Single document or other data
                if isinstance(data, dict):
                    # Format single document in a readable way
                    response = "ðŸ“Š RESULT:\n"
                    # Prefer a single-line summary first
                    if isinstance(data, dict):
                        response += render_line(data) + "\n\n"
                    # Then show key fields compactly (truncate long strings)
                    for key, value in data.items():
                        if isinstance(value, (str, int, float, bool)):
                            response += f"â€¢ {key}: {truncate_str(value, 140)}\n"
                        elif isinstance(value, dict):
                            # Show only shallow summary for dict
                            name_val = value.get('name') or value.get('title')
                            if name_val:
                                response += f"â€¢ {key}: {truncate_str(name_val, 120)}\n"
                            else:
                                child_keys = ", ".join(list(value.keys())[:5])
                                response += f"â€¢ {key}: {{ {child_keys} }}\n"
                        elif isinstance(value, list):
                            if len(value) <= 5:
                                response += f"â€¢ {key}: {truncate_str(str(value), 160)}\n"
                            else:
                                response += f"â€¢ {key}: [{len(value)} items]\n"
                    return response
                else:
                    # Fallback to JSON for other data types
                    return f"ðŸ“Š RESULTS:\n{json.dumps(data, indent=2)}"

            # Apply strong filter/transform now that we know the primary entity
            primary_entity = intent.get('primary_entity') if isinstance(intent, dict) else None
            filtered = filter_and_transform_content(filtered, primary_entity=primary_entity)

            # Format in LLM-friendly way
            max_items = None if show_all else 50
            formatted_result = format_llm_friendly(filtered, max_items=max_items, primary_entity=primary_entity)
            # If members primary entity and no rows, proactively hint about filters
            try:
                if isinstance(result.get("intent"), dict) and result["intent"].get("primary_entity") == "members" and not filtered:
                    formatted_result += "\n(No members matched. Try filtering by name, role, type, or project.)"
            except Exception:
                pass
            response += formatted_result
            print(response)
            return response
        else:
            return f"âŒ QUERY FAILED:\nQuery: '{query}'\nError: {result['error']}"

    except Exception as e:
        return f"âŒ INTELLIGENT QUERY ERROR:\nQuery: '{query}'\nError: {str(e)}"


@tool
async def rag_search(
    query: str,
    content_type: Optional[str] = None,
    group_by: Optional[str] = None,
    limit: int = 10,
    show_content: bool = True,
    use_chunk_aware: bool = True
) -> str:
    """Universal RAG search tool - returns FULL chunk content for LLM synthesis.
    
    **IMPORTANT**: This tool returns complete, untruncated content chunks so you can:
    - Analyze and understand the actual content
    - Generate properly formatted responses based on real data
    - Answer questions accurately using the retrieved context
    - Synthesize information from multiple sources
    
    Use this for ANY content-based search or analysis needs:
    - Find relevant pages, work items, projects, cycles, modules, epics
    - Search by semantic meaning (not just keywords)
    - Get full context for answering questions
    - Analyze content patterns and distributions
    - Group/breakdown results by any dimension
    
    **When to use:**
    - "Find/search/show me pages about X"
    - "What content discusses Y?"
    - "Which work items mention authentication?"
    - "Show me recent documentation about APIs"
    - "Break down results by project/date/priority/etc."
    
    **Do NOT use for:**
    - Structured database queries (counts, filters on structured fields) â†’ use `mongo_query`
    
    Args:
        query: Search query (semantic meaning, not just keywords)
        content_type: Filter by type - 'page', 'work_item', 'project', 'cycle', 'module', 'epic', or None (all)
        group_by: Group results by field - 'project_name', 'updatedAt', 'priority', 'state_name', 
                 'content_type', 'assignee_name', 'visibility', etc. (None = no grouping)
        limit: Max results to retrieve (default 10, increase for broader searches)
        show_content: If True, shows full content; if False, shows only metadata
        use_chunk_aware: If True, uses chunk-aware retrieval for better context (default True)
    
    Returns: FULL chunk content with rich metadata - ready for LLM synthesis and formatting
    
    Examples:
        query="authentication" â†’ finds all content about authentication with full text
        query="API documentation", content_type="page" â†’ finds API docs pages with complete content
        query="bugs", content_type="work_item", group_by="priority" â†’ work items grouped by priority
    """
    try:
        # Fix: Add project root to sys.path to resolve module imports
        # This makes 'qdrant' and 'mongo' modules importable from any script location.
        # Adjust the number of os.path.dirname if your directory structure is different.
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if project_root not in sys.path:
            sys.path.append(project_root)
        from qdrant.retrieval import ChunkAwareRetriever, format_reconstructed_results
    except ImportError as e:
        return f"âŒ RAG dependency error: {e}. Please ensure all modules are in the correct path."
    except Exception as e:
        return f"âŒ RAG SEARCH INITIALIZATION ERROR: {str(e)}"
    
    try:
        from collections import defaultdict

        # Ensure RAGTool is initialized
        try:
            rag_tool = RAGTool.get_instance()
        except RuntimeError:
            # Try to initialize if not already done
            await RAGTool.initialize()
            rag_tool = RAGTool.get_instance()
        
        # Resolve effective limit based on content_type defaults (opt-in when caller uses default)
        effective_limit: int = limit
        if content_type:
            default_for_type = CONTENT_TYPE_DEFAULT_LIMITS.get(content_type)
            if default_for_type is not None and (limit is None or limit == DEFAULT_RAG_LIMIT):
                effective_limit = default_for_type

        # Use chunk-aware retrieval if enabled and not grouping
        if use_chunk_aware and not group_by:
            from qdrant.retrieval import ChunkAwareRetriever, format_reconstructed_results
            
            retriever = ChunkAwareRetriever(
                qdrant_client=rag_tool.qdrant_client,
                embedding_model=rag_tool.embedding_model
            )
            
            from mongo.constants import QDRANT_COLLECTION_NAME
            
            # Per content_type chunk-level tuning
            chunks_per_doc = CONTENT_TYPE_CHUNKS_PER_DOC.get(content_type or "", 3)
            include_adjacent = CONTENT_TYPE_INCLUDE_ADJACENT.get(content_type or "", True)
            min_score = CONTENT_TYPE_MIN_SCORE.get(content_type or "", 0.5)

            from mongo.constants import RAG_CONTEXT_TOKEN_BUDGET
            reconstructed_docs = await retriever.search_with_context(
                query=query,
                collection_name=QDRANT_COLLECTION_NAME,
                content_type=content_type,
                limit=effective_limit,
                chunks_per_doc=chunks_per_doc,
                include_adjacent=include_adjacent,
                min_score=min_score,
                context_token_budget=RAG_CONTEXT_TOKEN_BUDGET
            )
            
            if not reconstructed_docs:
                return f"âŒ No results found for query: '{query}'"
            
            # Always pass full content chunks to the agent by default for synthesis
            # Force show_full_content=True so downstream LLM has full context
            return format_reconstructed_results(
                docs=reconstructed_docs,
                show_full_content=True,
                show_chunk_details=True
            )
        
        # Fallback to standard retrieval
        results = await rag_tool.search_content(query, content_type=content_type, limit=effective_limit)
        
        if not results:
            return f"âŒ No results found for query: '{query}'"
        
        # Build response header
        response = f"ðŸ” RAG SEARCH: '{query}'\n"
        response += f"Found {len(results)} result(s)"
        if content_type:
            response += f" (type: {content_type})"
        response += "\n\n"
        
        # NO GROUPING - Show detailed list with metadata
        if not group_by:
            response += "ðŸ“‹ RESULTS:\n\n"
            for i, result in enumerate(results[:15], 1):
                response += f"[{i}] {result['content_type'].upper()}: {result['title']}\n"
                response += f"    Score: {result['score']:.3f}\n"
                
                # Show metadata compactly
                meta = []
                if result.get('project_name'):
                    meta.append(f"Project: {result['project_name']}")
                if result.get('priority'):
                    meta.append(f"Priority: {result['priority']}")
                if result.get('state_name'):
                    meta.append(f"State: {result['state_name']}")
                if result.get('assignee_name'):
                    meta.append(f"Assignee: {result['assignee_name']}")
                if result.get('displayBugNo'):
                    meta.append(f"Bug#: {result['displayBugNo']}")
                if result.get('updatedAt'):
                    date_str = str(result['updatedAt']).split('T')[0] if 'T' in str(result['updatedAt']) else str(result['updatedAt'])[:10]
                    meta.append(f"Updated: {date_str}")
                if result.get('visibility'):
                    meta.append(f"Visibility: {result['visibility']}")
                if result.get('business_name'):
                    meta.append(f"Business: {result['business_name']}")
                
                if meta:
                    response += f"    {' | '.join(meta)}\n"
                
                # Always include FULL content for LLM synthesis (no truncation)
                # This enables the LLM to generate properly formatted responses based on actual content
                if result.get('content'):
                    content_text = result['content']
                    response += f"\n    === CONTENT START ===\n{content_text}\n    === CONTENT END ===\n"
                
                response += "\n"
            
            if len(results) > 15:
                response += f"... and {len(results) - 15} more results (increase limit to see more)\n"
            
            return response
        
        # GROUPING - Aggregate and show distribution with content snippets
        groups = defaultdict(list)
        
        for result in results:
            group_val = result.get(group_by)
            
            # Handle date grouping
            if group_by in ['createdAt', 'updatedAt'] and group_val:
                if isinstance(group_val, str):
                    group_val = group_val.split('T')[0] if 'T' in group_val else group_val[:10]
            
            # Handle None/empty
            if group_val is None or group_val == "":
                group_val = "Unknown"
            
            groups[str(group_val)].append(result)
        
        # Sort groups by count
        sorted_groups = sorted(groups.items(), key=lambda x: len(x[1]), reverse=True)
        
        response += f"ðŸ“Š GROUPED BY '{group_by}':\n"
        response += f"Total groups: {len(sorted_groups)}\n\n"
        
        for group_key, items in sorted_groups[:20]:
            response += f"â–¸ {group_key}: {len(items)} item(s)\n"
            
            # Show sample items with content snippets for context
            for item in items[:3]:
                title = item['title'][:55] + "..." if len(item['title']) > 55 else item['title']
                response += f"  â€¢ {title} (score: {item['score']:.2f})\n"
                # Include content snippet for better LLM understanding
                if show_content and item.get('content'):
                    snippet = item['content'][:200] + "..." if len(item['content']) > 200 else item['content']
                    response += f"    Content: {snippet}\n"
            
            if len(items) > 3:
                response += f"  ... and {len(items) - 3} more\n"
            response += "\n"
        
        if len(sorted_groups) > 20:
            remaining_items = sum(len(items) for _, items in sorted_groups[20:])
            response += f"... and {len(sorted_groups) - 20} more groups ({remaining_items} items)\n"
        
        return response
        
    except ImportError:
        return "âŒ RAG not available. Install: qdrant-client, sentence-transformers"
    except Exception as e:
        return f"âŒ RAG SEARCH ERROR: {str(e)}"


# Global websocket registry for content generation
_GENERATION_WEBSOCKET = None
_GENERATION_CONVERSATION_ID = None

def set_generation_websocket(websocket):
    """Set the websocket connection for direct content streaming."""
    global _GENERATION_WEBSOCKET
    _GENERATION_WEBSOCKET = websocket

def get_generation_websocket():
    """Get the current websocket connection."""
    return _GENERATION_WEBSOCKET

def set_generation_context(websocket, conversation_id):
    """Set websocket and conversation context for generated content persistence."""
    global _GENERATION_WEBSOCKET, _GENERATION_CONVERSATION_ID
    _GENERATION_WEBSOCKET = websocket
    _GENERATION_CONVERSATION_ID = conversation_id

def get_generation_conversation_id():
    """Get the current conversation id for generated content context."""
    return _GENERATION_CONVERSATION_ID


@tool
async def generate_content(
    content_type: str,
    prompt: str,
    template_title: str = "",
    template_content: str = "",
    context: Optional[Dict[str, Any]] = None
) -> str:
    """Generate work items, pages, cycles, modules, or epics - sends content DIRECTLY to frontend, returns minimal confirmation.
    
    **CRITICAL TOKEN OPTIMIZATION**: 
    - Full generated content is sent directly to the frontend via WebSocket
    - Agent receives only a minimal success/failure signal (no content details)
    - Prevents generated content from being sent back through the LLM
    
    Use this to create new content:
    - Work items: bugs, tasks, features  
    - Pages: documentation, meeting notes, project plans
    - Cycles: sprints, iterations, development cycles
    - Modules: feature modules, components, subsystems
    - Epics: larger initiatives spanning multiple work items
    
    Args:
        content_type: Type of content - 'work_item', 'page', 'cycle', 'module', or 'epic'
        prompt: User's instruction for what to generate
        template_title: Optional template title to base generation on
        template_content: Optional template content to use as structure
        context: Optional context dict with additional parameters (pageId, projectId, etc.)
    
    Returns:
        Minimal success/failure signal (NOT content details) - saves maximum tokens
    
    Examples:
        generate_content(content_type="work_item", prompt="Bug: login fails on mobile")
        generate_content(content_type="page", prompt="Create API documentation", context={...})
        generate_content(content_type="cycle", prompt="Q4 2024 Sprint")
        generate_content(content_type="module", prompt="Authentication Module")
        generate_content(content_type="epic", prompt="Customer Onboarding Epic")
    """
    import httpx
    
    try:
        if content_type not in ["work_item", "page", "cycle", "module", "epic"]:
            return "âŒ Invalid content type"
        
        # Get API base URL from environment or use default
        api_base = os.getenv("API_BASE_URL", "http://localhost:8000")
        
        if content_type == "work_item":
            # Call work item generation endpoint
            url = f"{api_base}/generate/work-item"
            payload = {
                "prompt": prompt,
                "template": {
                    "title": template_title or "Work Item",
                    "content": template_content or ""
                }
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                result = response.json()
            
            # Send full content directly to frontend (bypass agent)
            websocket = get_generation_websocket()
            if websocket:
                try:
                    await websocket.send_json({
                        "type": "content_generated",
                        "content_type": "work_item",
                        "data": result,  # Full content to frontend
                        "success": True
                    })
                except Exception as e:
                    print(f"Warning: Could not send to websocket: {e}")

            # Persist generated artifact as a conversation message (best-effort)
            try:
                conv_id = get_generation_conversation_id()
                if conv_id:
                    from mongo.conversations import save_generated_work_item
                    title = (result or {}).get("title") if isinstance(result, dict) else None
                    description = (result or {}).get("description") if isinstance(result, dict) else None
                    await save_generated_work_item(conv_id, {
                        "title": (title or "Work item").strip(),
                        "description": (description or "").strip(),
                        # Optional fields; may be filled later by explicit save to domain DB
                        "projectIdentifier": (result or {}).get("projectIdentifier") if isinstance(result, dict) else None,
                        "sequenceId": (result or {}).get("sequenceId") if isinstance(result, dict) else None,
                        "link": (result or {}).get("link") if isinstance(result, dict) else None,
                    })
            except Exception as e:
                # Non-fatal
                print(f"Warning: failed to persist generated work item to conversation: {e}")
            
            # Return MINIMAL confirmation to agent (no content details)
            return "âœ… Content generated"
            
        elif content_type == "cycle":
            # Call cycle generation endpoint
            url = f"{api_base}/generate/cycle"
            payload = {
                "prompt": prompt,
                "template": {
                    "title": template_title or "Cycle",
                    "content": template_content or ""
                }
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                result = response.json()
            
            # Send full content directly to frontend (bypass agent)
            websocket = get_generation_websocket()
            if websocket:
                try:
                    await websocket.send_json({
                        "type": "content_generated",
                        "content_type": "cycle",
                        "data": result,  # Full content to frontend
                        "success": True
                    })
                except Exception as e:
                    print(f"Warning: Could not send to websocket: {e}")

            # Persist generated artifact as a conversation message (best-effort)
            try:
                conv_id = get_generation_conversation_id()
                if conv_id:
                    from mongo.conversations import save_generated_cycle
                    title = (result or {}).get("title") if isinstance(result, dict) else None
                    description = (result or {}).get("description") if isinstance(result, dict) else None
                    await save_generated_cycle(conv_id, {
                        "title": (title or "Cycle").strip(),
                        "description": (description or "").strip(),
                    })
            except Exception as e:
                # Non-fatal
                print(f"Warning: failed to persist generated cycle to conversation: {e}")
            
            # Return MINIMAL confirmation to agent (no content details)
            return "âœ… Content generated"
            
        elif content_type == "module":
            # Call module generation endpoint
            url = f"{api_base}/generate/module"
            payload = {
                "prompt": prompt,
                "template": {
                    "title": template_title or "Module",
                    "content": template_content or ""
                }
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                result = response.json()
            
            # Send full content directly to frontend (bypass agent)
            websocket = get_generation_websocket()
            if websocket:
                try:
                    await websocket.send_json({
                        "type": "content_generated",
                        "content_type": "module",
                        "data": result,  # Full content to frontend
                        "success": True
                    })
                except Exception as e:
                    print(f"Warning: Could not send to websocket: {e}")

            # Persist generated artifact as a conversation message (best-effort)
            try:
                conv_id = get_generation_conversation_id()
                if conv_id:
                    from mongo.conversations import save_generated_module
                    title = (result or {}).get("title") if isinstance(result, dict) else None
                    description = (result or {}).get("description") if isinstance(result, dict) else None
                    await save_generated_module(conv_id, {
                        "title": (title or "Module").strip(),
                        "description": (description or "").strip(),
                    })
            except Exception as e:
                # Non-fatal
                print(f"Warning: failed to persist generated module to conversation: {e}")
            
            # Return MINIMAL confirmation to agent (no content details)
            return "âœ… Content generated"

        elif content_type == "epic":
            # Call epic generation endpoint
            url = f"{api_base}/generate/epic"
            payload = {
                "prompt": prompt,
                "template": {
                    "title": template_title or "Epic",
                    "content": template_content or ""
                }
            }

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                result = response.json()

            # Send full content directly to frontend (bypass agent)
            websocket = get_generation_websocket()
            if websocket:
                try:
                    await websocket.send_json({
                        "type": "content_generated",
                        "content_type": "epic",
                        "data": result,  # Full content to frontend
                        "success": True
                    })
                except Exception as e:
                    print(f"Warning: Could not send to websocket: {e}")

            # Persist generated epic as a conversation message (best-effort)
            try:
                conv_id = get_generation_conversation_id()
                if conv_id:
                    from mongo.conversations import save_generated_epic
                    epic_payload = {}
                    if isinstance(result, dict):
                        epic_payload = {
                            "title": result.get("title"),
                            "description": result.get("description"),
                            "priority": result.get("priority"),
                            "state": result.get("state") or result.get("stateName"),
                            "assignee": result.get("assignee"),
                            "labels": result.get("labels"),
                            "projectId": result.get("projectId"),
                            "startDate": result.get("startDate"),
                            "dueDate": result.get("dueDate"),
                            "link": result.get("link"),
                        }
                    await save_generated_epic(conv_id, epic_payload)
            except Exception as e:
                # Non-fatal
                print(f"Warning: failed to persist generated epic to conversation: {e}")

            # Return MINIMAL confirmation to agent (no content details)
            return "âœ… Content generated"
            
        else:  # content_type == "page"
            # Call page generation endpoint
            url = f"{api_base}/stream-page-content"
            
            # Build request context
            page_context = context or {}
            payload = {
                "prompt": prompt,
                "template": {
                    "title": template_title or "Page",
                    "content": template_content or ""
                },
                "context": page_context.get("context", {
                    "tenantId": "",
                    "page": {"type": "DOCUMENTATION"},
                    "subject": {},
                    "timeScope": {},
                    "retrieval": {},
                    "privacy": {}
                }),
                "pageId": page_context.get("pageId", ""),
                "projectId": page_context.get("projectId", ""),
                "tenantId": page_context.get("tenantId", "")
            }
            
            # Send as query param (matching the endpoint's expectation)
            import json as json_lib
            params = {"data": json_lib.dumps(payload)}
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
                result = response.json()
            
            # Send full content directly to frontend (bypass agent)
            websocket = get_generation_websocket()
            if websocket:
                try:
                    await websocket.send_json({
                        "type": "content_generated",
                        "content_type": "page",
                        "data": result,  # Full content to frontend
                        "success": True
                    })
                except Exception as e:
                    print(f"Warning: Could not send to websocket: {e}")

            # Persist generated page as a conversation message (best-effort)
            try:
                conv_id = get_generation_conversation_id()
                if conv_id:
                    from mongo.conversations import save_generated_page
                    # Ensure blocks shape
                    blocks = result if isinstance(result, dict) else {}
                    if not isinstance(blocks.get("blocks"), list):
                        blocks = {"blocks": []}
                    title = (result or {}).get("title") if isinstance(result, dict) else None
                    await save_generated_page(conv_id, {
                        "title": (title or "Generated Page").strip(),
                        "blocks": blocks
                    })
            except Exception as e:
                print(f"Warning: failed to persist generated page to conversation: {e}")
            
            # Return MINIMAL confirmation to agent (no content details)
            return "âœ… Content generated"
            
    except httpx.HTTPStatusError as e:
        error_msg = f"API error: {e.response.status_code}"
        # Send error to frontend
        websocket = get_generation_websocket()
        if websocket:
            try:
                await websocket.send_json({
                    "type": "content_generated",
                    "content_type": content_type,
                    "error": error_msg,
                    "success": False
                })
            except Exception:
                pass
        return f"âŒ {error_msg}"
    except httpx.RequestError as e:
        error_msg = "Connection error"
        websocket = get_generation_websocket()
        if websocket:
            try:
                await websocket.send_json({
                    "type": "content_generated",
                    "content_type": content_type,
                    "error": error_msg,
                    "success": False
                })
            except Exception:
                pass
        return f"âŒ {error_msg}"
    except Exception as e:
        error_msg = "Generation failed"
        websocket = get_generation_websocket()
        if websocket:
            try:
                await websocket.send_json({
                    "type": "content_generated",
                    "content_type": content_type,
                    "error": str(e)[:200],
                    "success": False
                })
            except Exception:
                pass
        return f"âŒ {error_msg}"


# Define the tools list - streamlined and powerful
tools = [
    mongo_query,              # Structured MongoDB queries with intelligent planning
    rag_search,               # Universal RAG search with filtering, grouping, and metadata
    generate_content,         # Generate work items/pages (returns summary only, not full content)
]

# import asyncio

# if __name__ == "__main__":
#     async def main():
#         # Test the tools    
#         while True:
#             question = input("Enter your question: ")
#             if question.lower() in ['exit', 'quit']:
#                 break

#             print("\nðŸŽ¯ Testing intelligent_query...")

#             print("ðŸ” Testing rag_content_search...")
#             result1 = await rag_content_search.ainvoke({
#                 "query": question,   # rag_content_search expects `query`
#             })
#             print(result1)

#             print("\nðŸ“– Testing rag_answer_question...")
#             result2 = await rag_answer_question.ainvoke({
#                 "question": question,   # rag_answer_question expects `question`
#             })
#             print(result2)

#     asyncio.run(main())
